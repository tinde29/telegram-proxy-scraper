import requests
from bs4 import BeautifulSoup
import re
import random
import time
import logging
from datetime import datetime
import pytz

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
]

def get_random_user_agent():
    return random.choice(USER_AGENTS)

def fetch_proxies_from_text_urls(urls):
    all_links = []
    headers = {'User-Agent': get_random_user_agent()}
    
    for url in urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            lines = response.text.splitlines()
            links = [line.strip() for line in lines if re.match(r'^tg://proxy\?server=.+&port=\d+&secret=.+$', line.strip())]
            all_links.extend(links)
            logging.info(f"Fetched {len(links)} proxies from {url}")
        except requests.RequestException as e:
            logging.error(f"HTTP error fetching {url}: {e}")
        time.sleep(random.uniform(1, 3))
    return all_links

def fetch_proxies_from_telegram_channel(url):
    proxies = []
    headers = {'User-Agent': get_random_user_agent()}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        proxy_elements = soup.find_all('a', href=re.compile(r'^tg://proxy\?server=.+&port=\d+&secret=.+$'))
        proxies = [element.get('href') for element in proxy_elements]
        logging.info(f"Fetched {len(proxies)} proxies from {url}")
    except requests.RequestException as e:
        logging.error(f"Error fetching data from {url}: {e}")
    time.sleep(random.uniform(2, 5))
    return proxies

def save_proxies_to_file(proxy_list, filename='proxy.txt'):
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            for proxy in proxy_list:
                file.write(proxy + '\n')
        logging.info(f"Saved {len(proxy_list)} proxies to {filename}")
    except IOError as e:
        logging.error(f"Error writing to {filename}: {e}")
    return proxy_list

def update_readme(proxy_list):
    try:
        utc_now = datetime.now(pytz.UTC)
        iran_tz = pytz.timezone('Asia/Tehran')
        iran_now = utc_now.astimezone(iran_tz)
        update_time_utc = utc_now.strftime('%d %B %Y, %H:%M UTC')
        update_time_iran = iran_now.strftime('%H:%M')

        sample_proxies = random.sample(proxy_list, min(30, len(proxy_list))) if proxy_list else []
        table_rows = ""
        for i, proxy in enumerate(sample_proxies, 1):
            match = re.match(r'tg://proxy\?server=([^&]+)&port=(\d+)&secret=.+$', proxy)
            if match:
                server, port = match.groups()
                table_rows += f"| {i} | {server} | {port} | ÙØ¹Ø§Ù„ | `{proxy}` |\n"

        readme_content = f"""# Telegram Proxy Scraper

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/python-3.9-blue)](https://www.python.org/downloads/)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/tinde29/telegram-proxy-scraper/issues)
[![Proxy Scraper Workflow](https://github.com/tinde29/telegram-proxy-scraper/actions/workflows/scraper.yml/badge.svg)](https://github.com/tinde29/telegram-proxy-scraper/actions/workflows/scraper.yml)
![GitHub last commit](https://img.shields.io/github/last-commit/tinde29/telegram-proxy-scraper)
![GitHub issues](https://img.shields.io/github/issues/tinde29/telegram-proxy-scraper)

**Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§**: {update_time_utc} (Ø¨Ù‡ ÙˆÙ‚Øª Ø§ÛŒØ±Ø§Ù†: {update_time_iran})

Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÙ‡ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…Ù‡. Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ØªÙˆ ÙØ§ÛŒÙ„ `proxy.txt` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´Ù† Ùˆ Ù‡Ø± 6 Ø³Ø§Ø¹Øª Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´Ù†.

## Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡

Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² `requests` Ùˆ `BeautifulSoup` Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto Ø±Ùˆ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ (Ù…Ø«Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨) Ùˆ ØµÙØ­Ø§Øª ÙˆØ¨ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… (`t.me/s/...`) Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡. Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´Ù† Ùˆ Ù†ØªÛŒØ¬Ù‡ ØªÙˆ ÙØ§ÛŒÙ„ `proxy.txt` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ **GitHub Actions** Ù‡Ø± 6 Ø³Ø§Ø¹Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù‡.

## ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
- Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
- Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù‡Ø± 6 Ø³Ø§Ø¹Øª
- Ø­Ø°Ù Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
- Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø³Ø±ÙˆØ± ÛŒØ§ API ØªÙ„Ú¯Ø±Ø§Ù…
- Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto ÙØ¹Ø§Ù„ Ù‡Ø³ØªÙ†

## Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
- Ù¾Ø§ÛŒØªÙˆÙ† 3.9
- Ú©ØªØ§Ø¨Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²: `requests`, `beautifulsoup4`, `pytz`
- ÙØ§ÛŒÙ„ `requirements.txt` Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§Ø³Øª.

## Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
1. ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ùˆ Ø§Ø² [Ø§ÛŒÙ†Ø¬Ø§](proxy.txt) Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.
2. Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ (Ø¨Ø§ ÙØ±Ù…Øª `tg://proxy?...`) Ø±Ùˆ ØªÙˆ Ú©Ù„Ø§ÛŒÙ†Øª ØªÙ„Ú¯Ø±Ø§Ù… Ø®ÙˆØ¯ØªÙˆÙ† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.
3. Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ±ØŒ Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú© ØªÙˆ Ø³ØªÙˆÙ† "Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ" Ù„Ù…Ø³ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ "Copy" Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ØŒ Ø³Ù¾Ø³ ØªÙˆ ØªÙ„Ú¯Ø±Ø§Ù… Ù¾ÛŒØ³Øª Ú©Ù†ÛŒØ¯.
4. Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø³ØªÛŒØŒ Ø¨Ù‡ ØªØ¨ **Actions** ØªÙˆ Ù…Ø®Ø²Ù† Ø¨Ø±ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ **Run workflow** Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯.

## Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
- **Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ**:
  - [MahsaNetConfigTopic](https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt)
  - [ALIILAPRO/MTProtoProxy](https://raw.githubusercontent.com/ALIILAPRO/MTProtoProxy/main/proxy-list.txt)
- **Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…**:
  - iporoto, HiProxy, iproxy, iRoProxy, proxyforopeta, IRN_Proxy, MProxy_ir, ProxyHagh, PyroProxy, ProxyMTProto, MTPro_XYZ, vpns, mtmvpn

## Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§
Ø¬Ø¯ÙˆÙ„ Ø²ÛŒØ± 30 Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ùˆ Ù†Ø´ÙˆÙ† Ù…ÛŒâ€ŒØ¯Ù‡. Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒØŒ Ø±ÙˆÛŒ Ù…ØªÙ† ØªÙˆ Ø³ØªÙˆÙ† "Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ" Ù„Ù…Ø³ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ "Copy" Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:

| #  | Ø³Ø±ÙˆØ± (Server)       | Ù¾ÙˆØ±Øª (Port) | ÙˆØ¶Ø¹ÛŒØª     | Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ                     |
|----|---------------------|-------------|-----------|---------------------------------|
{table_rows}

> **ØªÙˆØ¬Ù‡**: Ø§ÛŒÙ† Ø¬Ø¯ÙˆÙ„ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Øª. Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù‡Ù…Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²ØŒ ÙØ§ÛŒÙ„ [proxy.txt](proxy.txt) Ø±Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.

## Ù…Ø´Ø§Ø±Ú©Øª
Ø§Ø² Ù…Ø´Ø§Ø±Ú©Øª Ø´Ù…Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…! Ø§Ú¯Ù‡ Ø§ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¯Ø§Ø±ÛŒØ¯ ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒØ¯ Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯:
1. ÛŒÙ‡ **Issue** ØªÙˆ Ù…Ø®Ø²Ù† Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯.
2. ÛŒØ§ ÛŒÙ‡ **Pull Request** Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨ÙØ±Ø³ØªÛŒØ¯.

## Ù„Ø§ÛŒØ³Ù†Ø³
Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ­Øª [Ù„Ø§ÛŒØ³Ù†Ø³ MIT](LICENSE) Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡.

## Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯
- ğŸ“„ [Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§](proxy.txt)
- ğŸš€ [ÙˆØ¶Ø¹ÛŒØª GitHub Actions](https://github.com/tinde29/telegram-proxy-scraper/actions)
- â­ [Ù…Ø§ Ø±Ùˆ Ø³ØªØ§Ø±Ù‡ Ø¨Ø¯ÛŒØ¯!](https://github.com/tinde29/telegram-proxy-scraper)

## Stargazers Ø¯Ø± Ú¯Ø°Ø± Ø²Ù…Ø§Ù†
[![Stargazers over time](https://starchart.cc/tinde29/telegram-proxy-scraper.svg?variant=adaptive)](https://starchart.cc/tinde29/telegram-proxy-scraper)

---

Ø³Ù¾Ø§Ø³ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Telegram Proxy Scraper**! Ø§Ú¯Ù‡ Ø³Ø¤Ø§Ù„ÛŒ Ø¯Ø§Ø±ÛŒØ¯ØŒ ØªÙˆ Ø¨Ø®Ø´ Issues Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯. ğŸŒŸ
"""

        with open('README.md', 'w', encoding='utf-8') as file:
            file.write(readme_content)
        logging.info("Updated README.md with latest update time and proxy samples")
    except Exception as e:
        logging.error(f"Error updating README.md: {e}")

if __name__ == "__main__":
    text_urls = [
        "https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt",
        "https://raw.githubusercontent.com/ALIILAPRO/MTProtoProxy/main/proxy-list.txt",
    ]
    
    telegram_urls = [
        "https://t.me/s/iporoto",
        "https://t.me/s/HiProxy",
        "https://t.me/s/iproxy",
        "https://t.me/s/iRoProxy",
        "https://t.me/s/proxyforopeta",
        "https://t.me/s/IRN_Proxy",
        "https://t.me/s/MProxy_ir",
        "https://t.me/s/ProxyHagh",
        "https://t.me/s/PyroProxy",
        "https://t.me/s/ProxyMTProto",
        "https://t.me/s/MTPro_XYZ",
        "https://t.me/s/vpns",
        "https://t.me/s/mtmvpn"
    ]
    
    text_proxies = fetch_proxies_from_text_urls(text_urls)
    
    telegram_proxies = []
    for url in telegram_urls:
        proxies = fetch_proxies_from_telegram_channel(url)
        telegram_proxies.extend(proxies)
    
    all_proxies = list(set(text_proxies + telegram_proxies))
    
    all_proxies = save_proxies_to_file(all_proxies)
    
    update_readme(all_proxies)
